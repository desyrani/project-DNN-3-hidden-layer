**Overview**

This repository showcases the architecture, implementation, and performance analysis of a Deep Neural Network (DNN) with three hidden layers. The project explores how increasing the depth of a neural network affects learning capability, model accuracy, and generalization in various machine learning tasks.

**Key focus areas include:**

- Theoretical understanding of hidden layers in DNN.
- Implementations using Python, TensorFlow, and PyTorch.
- Applications in image recognition, binary classification, and regression tasks.
- Optimization techniques for deeper neural networks.

**Key Features**

**3 Hidden Layer Architecture:**

- Fully connected layers with varying numbers of neurons (customizable).
- Activation functions including ReLU, Sigmoid, and Tanh.

**Training and Evaluation:**

- Training on standard datasets such as MNIST, CIFAR-10, and custom datasets.
- Metrics evaluation: Accuracy, loss, and convergence speed.

**Optimization:**

- Experimentation with optimizers like Adam, SGD, and RMSprop.
- Regularization techniques to prevent overfitting, such as dropout and L2 regularization.

**Visualization:**

- Loss curves and accuracy plots.
- Layer-wise weight and gradient analysis.
